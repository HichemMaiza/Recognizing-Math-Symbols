{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import inkml\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow.python.framework import ops\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths \n",
    "root_path = 'MasterVICO/DataSymbol_Iso/task2-trainSymb2014'\n",
    "\n",
    "training_Symbols = os.path.join(root_path, 'trainingSymbols')  \n",
    "training_Junks = os.path.join(root_path, 'trainingJunk')\n",
    "training_labels_Symbols = 'MasterVICO/DataSymbol_Iso/task2-trainSymb2014/trainingSymbols/iso_GT.txt'\n",
    "training_labels_Junks = 'MasterVICO/DataSymbol_Iso/task2-trainSymb2014/trainingJunk/junk_GT.txt'\n",
    "validation_labels_Symbols = 'MasterVICO/DataSymbol_Iso/task2-validation-isolatedTest2013b/iso_GT.txt'\n",
    "test_labels_Symbols = 'MasterVICO/DataSymbol_Iso/task2-testSymbols2014/testSymbols_2016_iso_GT.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder):\n",
    "    \n",
    "    image_size = 28  # Pixel width and height.\n",
    "    pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n",
    "    print(folder)\n",
    "    \n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        image_data = (imageio.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "        dataset[num_images, :, :] = image_data\n",
    "        num_images = num_images + 1\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(file) :  \n",
    "    Ground_Truth = []\n",
    "    with open (file) as f:\n",
    "        lines = f.read().split('\\n')[:-1]\n",
    "        for line in lines : \n",
    "            line = line.split(',')\n",
    "            Ground_Truth.append(line[1])\n",
    "            \n",
    "    return np.array(Ground_Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(dataset): \n",
    "    dataset = dataset.reshape((-1, 28, 28, 1)).astype(np.float32)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_dataset = load_images('data_png_trainingSymbols') \n",
    "#training_junks = load_images('data_png_trainingJunk') \n",
    "#validation_dataset = load_images('data_png_task2-validation-isolatedTest2013b')\n",
    "#test_dataset = load_images('data_png_testSymbols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('training_dataset', training_dataset) \n",
    "#np.save('training_junks', training_junks) \n",
    "#np.save('validation_dataset', validation_dataset) \n",
    "#np.save('test_dataset', test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_Symbols_labels = load_labels(training_labels_Symbols)\n",
    "#training_junks_labels  = load_labels (training_labels_Junks)\n",
    "#validation_symbols_labels = load_labels(validation_labels_Symbols)\n",
    "#test_symbols_labels =  load_labels(test_labels_Symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('training_Symbols_labels', training_Symbols_labels) \n",
    "#np.save('training_junks_labels', training_junks_labels) \n",
    "#np.save('validation_symbols_labels', validation_symbols_labels) \n",
    "#np.save('test_symbols_labels', test_symbols_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = np.load('training_dataset.npy') \n",
    "training_junks = np.load('training_junks.npy') \n",
    "validation_dataset = np.load('validation_dataset.npy') \n",
    "test_dataset = np.load('test_dataset.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of training_dataset is  (85802, 28, 28)\n",
      "the shape of training_junks is  (74284, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"the shape of training_dataset is \", training_dataset.shape ) \n",
    "print(\"the shape of training_junks is \" , training_junks.shape ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = np.append(training_dataset,training_junks, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_Symbols_labels = np.load('training_Symbols_labels.npy')\n",
    "training_junks_labels  = np.load ('training_junks_labels.npy')\n",
    "validation_symbols_labels = np.load('validation_symbols_labels.npy')\n",
    "test_symbols_labels =  np.load('test_symbols_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Junks_labels to training data symbols_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.append(training_Symbols_labels, training_junks_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keep track of the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the data symbols  is  (160086, 28, 28)\n",
      "the shape of the data labels is  (227623,)\n"
     ]
    }
   ],
   "source": [
    "print(\"the shape of the data symbols  is \", train_corpus.shape) \n",
    "print(\"the shape of the data labels is \"  , train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of  X_train  =  (160086, 28, 28, 1)\n",
      "the shape of Y_train  =  (227623, 102)\n",
      "\n",
      "\n",
      "\n",
      "the shape of X_validation  =  (12504, 102)\n",
      "the shape of  X_test =  (18435, 102)\n",
      "\n",
      "\n",
      "\n",
      "the shape of X_test  =  (18435, 28, 28, 1)\n",
      "the shape of Y_test  =  (18435, 102)\n"
     ]
    }
   ],
   "source": [
    "lb_model = LabelBinarizer() \n",
    "lb_model.fit(train_labels)\n",
    "Y_train = lb_model.transform(train_labels)\n",
    "Y_validation = lb_model.transform(validation_symbols_labels)\n",
    "Y_test = lb_model.transform(test_symbols_labels)\n",
    "\n",
    "X_train = reformat(train_corpus)\n",
    "X_validation = reformat(validation_dataset) \n",
    "X_test = reformat(test_dataset)\n",
    "\n",
    "# keep track of the shapes \n",
    "print (\"the shape of  X_train  = {}\".format(X_train.shape))\n",
    "print (\"the shape of Y_train  = {}\".format(Y_train.shape))\n",
    "print('\\n\\n')\n",
    "print (\"the shape of X_validation  = {}\".format(Y_validation.shape))\n",
    "print (\"the shape of  X_test = {}\".format(Y_test.shape))\n",
    "print('\\n\\n')\n",
    "print (\"the shape of X_test  = {}\".format( X_test.shape))\n",
    "print (\"the shape of Y_test  = {}\".format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define place holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placehoders(batch_size = 1024 , name = 'palce_holders') :\n",
    "    '''\n",
    "    a Fnction To define Input and output placeholders\n",
    "    arg -- \n",
    "    return -- \n",
    "    '''\n",
    "    with tf.name_scope(name) : \n",
    "    \n",
    "        X = tf.placeholder(dtype = 'float', shape = [batch_size,28,28,1], name = 'X')\n",
    "        Y = tf.placeholder(dtype = 'float', shape = [batch_size,102], name = 'Y' ) \n",
    "    \n",
    "    return X,Y                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_variables(num_hidden = 64, num_labels = 102, patch_size = 5, depth =16, \n",
    "                         image_size = 28, name = 'Variables'):\n",
    "    \n",
    "    \"\"\"A function to initialize variables\n",
    "    args -- \n",
    "    returns -- parameters: dict     \n",
    "    \"\"\"\n",
    "    with tf.name_scope(name) : \n",
    "    \n",
    "        W1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, 1, depth],stddev=0.1))\n",
    "        b1 = tf.Variable(tf.zeros([depth]))\n",
    "        W2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth],stddev=0.1))\n",
    "        b2 = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "        W3 = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "        b3 = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "        W4 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))    \n",
    "        b4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "        parameters = {\n",
    "                      \"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2,\n",
    "                      \"W3\": W3,\n",
    "                      \"b3\": b3,\n",
    "                      \"W4\": W4,\n",
    "                      \"b4\": b4\n",
    "                     }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forword Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forword_pass(data, parameters, name = 'forword_pass') :\n",
    "    \n",
    "    \"\"\"define the forword pass, system architecture\n",
    "    args -- \n",
    "    returns --  \n",
    "    \"\"\"\n",
    "    # defines weights and bias\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "\n",
    "    # define system architecture \n",
    "    with tf.name_scope(name) : \n",
    "    \n",
    "        conv = tf.nn.conv2d(data, W1, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + b1)\n",
    "        conv = tf.nn.conv2d(hidden, W2, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + b2)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, W3) + b3)\n",
    "        out = tf.add(tf.matmul(hidden, W4), b4)\n",
    "        \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backword Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backword_pass(logits, tf_train_labels, name = \"compute_loss\"):\n",
    "    \n",
    "    with tf.name_scope(name): \n",
    "    \n",
    "    #logits = model(tf_train_dataset)\n",
    "        loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    define accuracy \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (X_train, Y_train, X_validation, Y_validation ,X_test, Y_test, patch_size = 5,\n",
    "           learning_rate=0.0001 , num_epochs = 1, batch_size = 1024,depth = 16, num_hidden = 64) : \n",
    "    \n",
    "    '''main model function \n",
    "    args -- \n",
    "    returns -- \n",
    "    '''\n",
    "    #ops.reset_default_graph()\n",
    "    costs = [] \n",
    "    \n",
    "    \n",
    "    X, Y = create_placehoders()\n",
    "    parameters = initialize_variables()\n",
    "    logits = forword_pass(X, parameters)\n",
    "    \n",
    "    # define prediction \n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(forword_pass(X_validation,parameters))\n",
    "    test_prediction = tf.nn.softmax(forword_pass(X_test, parameters))\n",
    "    \n",
    "    # cost\n",
    "    cost = backword_pass(logits, Y)\n",
    "    \n",
    "    with tf.name_scope('train'): \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)    \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    print(\"parameters initialized \")\n",
    "    with tf.Session() as sess: \n",
    "    \n",
    "        sess.run(init) \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            offset = (epoch * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = X_train[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = Y_train[offset:(offset + batch_size), :]\n",
    "            feed_dict = {X : batch_data, Y : batch_labels}\n",
    "            costs.append(cost)\n",
    "            _, l, predictions = sess.run([optimizer, cost, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 50 == 0):\n",
    "                \n",
    "                print('Minibatch loss at epoch %d: %f' % (epoch, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), Y_validation))\n",
    "                \n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters initialized \n",
      "Minibatch loss at epoch 0: 5.216441\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.4%\n"
     ]
    }
   ],
   "source": [
    "model (X_train, Y_train, X_validation, Y_validation ,X_test, Y_test, patch_size= 5,\n",
    "           learning_rate = 0.0001, num_epochs = 1 , batch_size = 1024, \n",
    "           depth = 16, num_hidden = 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
